{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_2_1_Assignment_Polynomial_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MTd7W7TxCV7q"
   },
   "source": [
    "<hr>\n",
    "<h1 align=\"center\"><a href='https://shangeth.com/courses/'>Deep Learning - Beginners Track</a></h1>\n",
    "<h3 align=\"center\">Instructor: <a href='https://shangeth.com/'>Shangeth Rajaa</a></h3>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "es9QfSb6J8za"
   },
   "source": [
    "# Task-1 : Linear Regression on Non-Linear Data\n",
    "\n",
    "- Get X and y from dataset() function\n",
    "- Train a Linear Regression model for this dataset.\n",
    "- Visualize the model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-IlItpAKWBl"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Call ```dataset()``` function to get X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "_Qjqkxe7KXkh",
    "outputId": "e3648df2-f581-4cfe-fd9b-e9c6d257d088"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAD8CAYAAACPWyg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X+QHPV55/H3s6MRzJILu9g6DIMU\nFEcRhaIgBQWUU+7KYAcJ+zCLDAhin3WJL0oqUHUQl84iTlnExkGJysHnlO0rElPBFQLCYAvF4MgY\nlPMVKWGkkwTISIf4ZTTIRkEsvrM2aLX73B/Ts/TOds9Mz0zPz8+ramtnvtPT8+0V9DPfX8/X3B0R\nEZEkBtpdARER6T4KHiIikpiCh4iIJKbgISIiiSl4iIhIYgoeIiKSmIKHiIgkpuAhIiKJKXiIiEhi\ns9pdgbS8+93v9nPPPbfd1RAR6Sq7d+/+F3efU+24ng0e5557Lrt27Wp3NUREuoqZvVLLceq2EhGR\nxBQ8REQkMQUPERFJTMFDREQSU/AQEZHEena2lYhIv9i6p8Dm7Qd5bXSMs4dyrF+5kJGl+VQ/U8FD\nRKSLbd1T4JZvPsPY+AQAhdExbvnmMwCpBhB1W4mIdLHN2w9OBY6SsfEJbr5/L1v3FFL7XAUPEZEu\n9troWGS5O6x/YF9qAUTBQ0Ski509lIt9bXzC2bz9YCqfqzEPEZEuETUwvn7lQm7asjf2PXEtk0ap\n5SEi0gVKA+OF0TGc6QPjw4PZ2PdVapk0oinBw8zuMrPXzezZUNmtZlYws73BzwdDr91iZofM7KCZ\nrQyVrwrKDpnZhlD5fDN7MijfYmazm1FvEZFuETcwfuu2/bhHvyebMdavXJhKfZrV8vhbYFVE+R3u\nviT4eQTAzM4HrgMWBe/5ipllzCwDfBm4HDgfuD44FuDPg3P9EvAm8Ikm1VtEpCvEdT+Njo0zOjY+\no3x4MMvmqy9IbbpuU4KHu38fOFbj4VcC97n72+7+EnAIuCj4OeTuL7r7CeA+4EozM+BS4IHg/XcD\nI82ot4hIt0jS/ZQfyrHnM5d19TqPG83s6aBbazgoywOvho45HJTFlb8LGHX3k2XlIiJ945Lzqu7P\nNKWQ0iB5WJrB46vAe4ElwBHgCyl+FgBmts7MdpnZrqNHj6b9cSIiLbPjQLJ7WpoLBCHFqbru/pPS\nYzP7a+DbwdMCMDd06DlBGTHlbwBDZjYraH2Ejy//zDuBOwGWLVsWM4QkItIdwlNzk97QNm8/2J3d\nVmZ2VujpVUBpJtY24DozO8XM5gMLgB8ATwELgplVsykOqm9zdwd2AFcH718LPJRWvUVEOkH51Nyk\n0lrfUdKUloeZ3Qu8D3i3mR0GNgLvM7MlgAMvA78P4O77zex+4IfASeAGd58IznMjsB3IAHe5+/7g\nIz4F3GdmtwF7gK81o94iIp0qampuEmmt7yhpSvBw9+sjimNv8O7+eeDzEeWPAI9ElL9IcTaWiEhf\naKTlkOb6jhKtMBcR6UCNtBzSXN9RouAhItKB1q9ciNXxvvxQLvXAAQoeIiIdaWRpPvFAeS6bSb27\nqkTBQ0SkQ+UTdF2Zwe2rF7ek1QEKHiIiHWv9yoXkspmajp01UE8nV/0UPEREOtTI0jy3r15MfiiH\nUWyJfGz5PCwiTqS58VMUbQYlItJhojZ9guLaj3t2/ih2LCTthYFhCh4iIh2ktLK8tECwMDrG+m/s\nAyu2Lio5PRe/KVSzKXiIiHSQqJXl45O1zbuK6s5Ki8Y8REQ6SCNdT6PHZ24KlRYFDxGRDtLIyvK0\n81mFqdtKRKQFygfBLzlvDjsOHJ3xvDA6hkHiBYKtyGcVZh63c3qXW7Zsme/atavd1RARmTEI3mzD\ng1k2XrGoKQsEzWy3uy+rdpxaHiIiKWs0vXq5bMZakvywEo15iIikrJnrLwYM1vz63LYGDlDwEBFJ\nXTMHsicdHtxdSH2P8moUPEREUpYkR1UtxsYnWpqKJIqCh4hIyspzVA3lsnXt1RHWylQkURQ8RERa\nYGRpnic2XModa5bw9snJxFNxy7VyTUcUzbYSEWmCqGSGUYPazZh51cpNn+IoeIiINCgqmeEt33wG\nYEYAabS7KV8hMLWSuq1ERBoU1ZqIG9RupLspP5TjiQ2Xtj1wQJOCh5ndZWavm9mzobIzzOxRM3s+\n+D0clJuZfcnMDpnZ02b2a6H3rA2Of97M1obKLzSzZ4L3fMmslbkjRUQqi2tNRJXXO/OqE7qqwprV\n8vhbYFVZ2QbgMXdfADwWPAe4HFgQ/KwDvgrFYANsBC4GLgI2lgJOcMzvhd5X/lkiIm0T15pwYMWm\nx6etyRhZmucjF+YTzbbKmLV0f/JaNCV4uPv3gWNlxVcCdweP7wZGQuVf96KdwJCZnQWsBB5192Pu\n/ibwKLAqeO3n3X2nFxNxfT10LhGRtqvUmiiMjnHTlr0s/ex3p4LIw08fSTTbatK9owIHpDtgfqa7\nHwke/xg4M3icB14NHXc4KKtUfjiifAYzW0exNcO8efMarL6ISG1KN/bN2w9SiOnCevP4ODdt2csf\nf/Npjo9PJjp/u6flRmnJgHnQYkg9fa+73+nuy9x92Zw5c9L+OBGRKaV1HNVUCxzl3VmdNtZRkmbw\n+EnQ5UTw+/WgvADMDR13TlBWqfyciHIRkY7yJ1ufaej9X1yzhDvWLJlaiZ4fynXcWEdJmt1W24C1\nwKbg90Oh8hvN7D6Kg+NvufsRM9sO/FlokPwy4BZ3P2ZmPzWz5cCTwMeBv0qx3iIidbn3yVerHxRj\nKJedChKdGCzKNSV4mNm9wPuAd5vZYYqzpjYB95vZJ4BXgGuDwx8BPggcAo4DvwMQBInPAU8Fx33W\n3UuD8H9IcUZXDvhO8CMi0lEm6txcL5fNcOuHFzW5NulqSvBw9+tjXnp/xLEO3BBznruAuyLKdwG/\n0kgdRUTSljFLHEA6cRpuLbTCXESkSa6/eG71gyLe022BAxQ8RESa5raRxZw2O9nq8R0HjqZUm3Qp\neIiINNHxE8ky5rZ7X456KXiIiDRR0gV9nbgAsBYKHiIiCW3dU2DFpseZv+HhGbmrkiQ+zGasIxcA\n1kL7eYiIJFBt747S4Pet2/YzOjZe8VynzZ7VlYPloJaHiEgitezdMbI0z96Nl7HivWdUPNfo2Pi0\nVks3UfAQEUkgyd4d9/zeb/Cx5ZWTtN7yzWe6MoAoeIiIJBA3wB1XftvIYr64ZknsOEjcjoOdTsFD\nRKRMPQPix0+cjG1BjCzNc/vqxbGf143Tdc3rzMXS6ZYtW+a7du1qdzVEpMuUD4hDMfdUOIXI1j2F\nqgPiw4NZNl6xaNqA+IpNj0fu91Ham7wTmNlud19W7Ti1PEREQmodED/tlMqTVd88Ps76B/ZVbbV0\n6n4d1WiqrohISFwXUmF0jHM3PEx+KMf6lQtr6moan3A2bz84I9X65u0HeW10jLODc3XjdF0FDxER\nil1Rm7cfrLrlaWlP8vId/+KUB5nwWpBupuAhIn2rFDAKo2MYyfbKrvXYbk0/Uo2Ch4j0pfKB8TSm\nDnVz+pFqFDxEpC9FDYw3U9Rsq16i4CEifSnNtRUG7PnMZamdvxNoqq6I9KU0xyJ6dZwjTMFDRPrS\nue9K5wZvwCXnzUnl3J1EwUNE+s6fbH2GJ144lsq5HXhwd6Erkx0mkXrwMLOXzewZM9trZruCsjPM\n7FEzez74PRyUm5l9ycwOmdnTZvZrofOsDY5/3szWpl1vEelelXJTAdz75KtVz5ExwyimDhmodVFH\noFuTHSbRqpbHJe6+JJQvZQPwmLsvAB4LngNcDiwIftYBX4VisAE2AhcDFwEbSwFHRCSsNAW3MDqG\n886ivqWf/e5UEJmoktMvl83whWsv4I41S/jZ2yeZrGMebzcmO0yiXbOtrgTeFzy+G/gn4FNB+de9\nmK1xp5kNmdlZwbGPuvsxADN7FFgF3NvaaotIp4ubgvvm8fGpHf8yZrEBpJR+ZNcrx/i7nT+q+FkZ\nM34+N4s3j89MkNjrg+ataHk48F0z221m64KyM939SPD4x8CZweM8EG5PHg7K4spFRKap9I1/bHyC\nm7bsjQ0cmYF3FvXdUyVwlFonG69Y1DPJDpNoRcvjN929YGb/FnjUzA6EX3R3N7OmLO4MgtM6gHnz\nKu/eJSK96eyhXGTa81pMTDo3bdlLxqzqivOPXJifSnB4ei7LqdkBRo+Pd3WywyRSDx7uXgh+v25m\n36I4ZvETMzvL3Y8E3VKvB4cXgLmht58TlBV4p5urVP5PEZ91J3AnFPfzaO6ViEiaSnmmGs02u37l\nwhn7cSRVbUxkKJflwd2Fqc8YHRsnl81wx5olPR80SlLttjKz08zs35QeA5cBzwLbgNKMqbXAQ8Hj\nbcDHg1lXy4G3gu6t7cBlZjYcDJRfFpSJSA+IGuSuZW/vqFlVpV37hnLZVOqazRhmVN3zo9el3fI4\nE/iWmZU+6+/d/R/N7CngfjP7BPAKcG1w/CPAB4FDwHHgdwDc/ZiZfQ54Kjjus6XBcxHpfpU2YIr7\nJl+e2LAUcOCdfTPWP7CP8YnmdUKcNjvD569azM1b9ka+3uszrMJSDR7u/iJwQUT5G8D7I8oduCHm\nXHcBdzW7jiLSfnE33Uo342oBZ/P2g00LHPmybrRSGvdyvT7DKkwrzEWk7eJuupVuxtUCTrNaAQPG\njPGXXtpOtl4KHiLSdvXcjKsFnGa1AiYdbt22f1pZaVwlP5SbWoV+++rFfTNYDkrJLiIdoJ69vaNm\nVWUHjOMnTjJ/w8Ocmm3ed+PRsZmLAHtlO9l6KXiISEdIcjMuTesdG5+YWi0+lMvysxMnp1Z7j41P\nplndvqfgISJdpXyW1YQ7uWyG8YnJps6sChseTGfabzdT8BCRlql1IWDccVv3FPjk/ftmLOJLczvZ\nbMbYeMWi1M7frcyrrKTsVsuWLfNdu3a1uxoiPafeleDlLQYoDoqXDzRHHdcu5VN0+4GZ7Q5lQI8/\nTsFDRGpVLQBEBRaIXxcBxVQfp50ya+o9o8dP8LMTnRE4nthwabur0XK1Bg91W4l0uGblfGrGuSst\nzANmrPhe/419YFQcixgdG5+azVRvQsNaGcxIeDiYHWB80qfVsd/WbNRDLQ+RDhbXhTM8mGXjFYti\nv+03sxspbP6GhyOzzRqNZbNthVw2w0cuzPPtfUemglXp7wjJpgn3MrU8RHpAtY2Ndr1ybFp216j8\nTknOXS2fVFyAOHso19F5ncJB4sHd7yRbLP0db1+9uC+7qBqh4CHSAeJaD9U2Nrr3yVcjZx5VCgAl\n1dJ7lNfpkvPmcOxnb0e+55Lz5rDjwNGObXkMzp7FyNI8KzY9njhgSjQFD+lZtXTnpDmekKSecdlh\nq3UFxe07UUsroFIrIqpOlbZk3XHgaFP20UhLtXxXndxq6lTKbSU9qZb9IbbuKbD+G/umHbP+G/uq\n7iHRbJW6j9avXEh2wGLfm7Ho12rJ61Qpn1Rcd1mc10bHpuV7qlS3dhgwY/6Ghxlo4O8l02nAXHrS\nik2PV/zGPjyY5e3xCY5HpLAYymXZu/GyNKs3rcUT93+gAS9t+hBLP/vdqZQb5a9/dPm8aWMeUH3Q\nO64e4ZZX3MB4nFx2gDNOO4XXRsc4NTvQValBkvy9+oEGzKVn1NO1VK0bIupmXBKVBK/R+oTfVxgd\ni5wyWq70bXg0pq4O3DaymGW/cEbdXW9x+aSSzpwaG5+cOr4bAkfGjEn3vp9Z1QgFD+lotewWF3Uz\nT2vaaC31qeV91QJHqfto654CA0Hiv3Kl7qHyjLSlNRf13hC37inEDoz3ikl3Xtr0oXZXo6up20o6\nWqXup3wwAyiq2+YjF+ZnlNeqNK0z6tt8XH0qrUaOy8cUpzxLbNQCu/JV3UnXa8QpjQONT6ZzXxiw\n4v4YNR8PZDJWd8LD4cFsZCuzX1eP10LpSRQ8ekK1vve47p9STqJbt+2v2g0Vls0Ya3597ozAU0s3\n08ubPhQ5vbXWIJbLDvCv45NVPydjxvUXz2XHgaO8NjpWsWVSfoOs1uVWbayoUdkBSNKrlRkwls8f\nZueLbzLhTsaM5b84zD+/cKzq36n030CzAmu/UPBQ8OgJjdzM8lW6rjIDxvUXvXMTLt1MK+VhqqY8\nyNQSdOqRy2ZqCkhfXLNkWvde3FTaUiuu0nTcTlFqWW75wauxLaRq+bYUOOIpeCh49IR6M6zWctPO\nDhibr7lgxthJpy50K8nEtDSiZDPG5qsvqNjlllS1oNwK+VCgf210jNNzWcyKkwsUIBqj2VbSMSpl\nWq3l2+ApswYSB49abq3jkz61sriT0oBXYsQvDIwyPuF88v593LRlb1M+v9Q1d/OWvam0qGpVWlei\nANE+XRM8zGwV8N+BDPA37r6pzVWSGkTNTirPtFoq+9N/2M+bx8drGjBultKU3j/9h/0dHzigvi6w\nJMGmktLsrs3bDzYUOAywhAPn5bSor/26IniYWQb4MvBbwGHgKTPb5u4/bG/NpFx5K+P4iZMzbspR\n/dTjkz41K6Z0s0sy0F2vUiqOSus+pKgwOsZ7b3mk4WD00eXzePjpI3X/zZUuvTN0S3qSi4BD7v6i\nu58A7gOubHOdpMyfbH2Gm7fsnZbuox035Vx2gFoSY2QHjOMnTjatS6dWpW/wnZO8o3aNBI6MGR9b\nPo/bRhbHLnyEYtfYF9csmZE6BYpTbzVTqjN0RcsDyAOvhp4fBi4uP8jM1gHrAObNm9eamvWhuDGM\ne3b+qK394FAMCCcnvWLKDw9+h1s7rRKePpv2tNhOUZrxVfrvppRjKsnCRw2Cd56umG1lZlcDq9z9\nvwTP/xNwsbvfGPcezbZKR9yCtFOzAzXfiLMDlsoitFpnIaU1fbZWQ8HMoH7pKisNslebkKD1F52h\n12ZbFYC5oefnBGXSYnEZYGsdbM5YOoEDau9SaffXpVaM5XSauCy9yjHVvboleDwFLDCz+RSDxnXA\nb7e3Sv1n655Cw90szZr5I91heDALxCeqVI6p7tUVA+bufhK4EdgOPAfc7+7721ur/lLqdmiFjy2f\nFzlY2i9KN9xuqEOlG0g2Y1Nbv8ZNrdWU2+7VFcEDwN0fcfdfdvf3uvvn212ffpN0c6BGbHnqVc4Z\nPnVGuZX97kX5oRx7PnPZ1IyjfB0316FctqHgO5TLsuczl1X97MHsAJlM9L9Gfig3tbIdKm88Jd2p\na4KHtFcrt+kcn3Cef/1nM8qdYh95r3Z8hW+m4RltQ7ks2ZibdNQ5bv3womk7+pW/M5fNxLYsDLj1\nw8XWQtQNP+ztkx65gLM0oyw8fhHeZdCCYzQ43t26ZcxDmixpsri4/TGS5FlqhlZ+lgGn57KcOBm9\n42AjTpud4WcnJqb+fvnQv0H5zKTRsXGyA8bwYHYqd1NUtl4DPnLh9JQdpVxd5Z9zc8zaFuedabKl\n33HrYJLun650Ir1FwaMP1bOh0fqVC1n/wL5p3zRL6csrZTftVuXpzLfuKcxI7550b4qwocHZfP6q\n6IAd1UU4PukMzp7Fns8Ut8ddsenxGcc4sOPA0an6hv+NJ9ynWjYjS/MVE0Cu2PT41HGVjo374qBx\njP6gbqs+FDfdtrQDXayy+8T4hPPS0f/HrBq7VNolY8nqF9UXP7I0z96NxbGI0s9fXjtzFXR2wGrq\nYioF7K17Zs44j/vmHi6vdky1f+NKXVLldYsbr7j+4rkax+hjann0oWo3nnBq8tK3y7hvmU+8cCzV\nujYqvPCs0oruqO6jaqJWQV9y3hy+ve/IVAtlMDvAKdlM5ILA8M08fI6hmN3vwt/o47oRS8dU+zcO\n1z3qPKW6hbuaoro5G9k/XbpbV6wwr4dWmMertJVq1M5r3azaZkjNXNVc6fxJUphnB2xa1uGoela7\nlqWf/W5kABoezE51fZXE7dZooDUYfajWFebqtupDlaZNtnJKbtryQ7m6Zvxs3VNgxabHmb/hYVZs\nejyyaylKpa6iJOMA45POabNnVaxntWuJ+04YVa41GFIPdVv1gaiZVbevXhzZ3RA3C6fVSvmnqs3m\nKm1JWj7zKK7vvdqMn3omE5TEdRXVsyr/rbFx9m68rOIxla7lrZgUKFHlcft8a+xCKlHw6HFRN8Pw\n1Mvhwey0fuq4/vZWc4qp1U+ZlZmRC6oUWPIp9L1Xaj1UO1/cOEQ9Gv3WX21MJEwZbKUeCh49rlo3\n1JvHx1n/wD6geBOpNgT2seXz2HHgaOwAca2yA8bPnTqL0ePjsWMBY+OTjJWtrxgezLLxikUzbmzN\nWkNQy0ynOM0aL2rGt/6krQmtwZCkFDx6XC03vfEJ59Zt+9m8/WDFIGAGy37hDG4bWTxVtnVPgR0H\njvLW2DuL10rBpVIc2nzNO6krkuxrMTh7Vqo3uSTf2MuVf4OvZypKXHBMSq0JSZtmW/W4Zm84ZMC/\ne+8ZvPzGGIXRsRl7Y9QyNTZqAV6tu/mlPQOomTOy4q6/lIb89GBfj9Kqcd3cpRP02n4eUqdSKopm\nfUVwpq/tKD9veHyg1q6TkaV5/vQf9tc01pL2DKBmfmOPu37ldJJeoODRw0qzrFrdtoxaiFbtRrzx\nikU17TTXihlAzer/V9eR9DIFjx5Vy7afaQm3Dmq9Ecet1g4PznfjjVcD0dKrFDx6VDsX+9XbOtCN\nVqR7KHj0kPBiwHZNgxjKZRUARPqAgkePaGc3VVhpIyER6W0KHl0s3NIYaPGmTFHU6hDpHwoeHS5u\nx7+ozX7aqbT9qYj0BwWPDlYpSV8nZb/NmM3Y/lREeltqKdnN7FYzK5jZ3uDng6HXbjGzQ2Z20MxW\nhspXBWWHzGxDqHy+mT0ZlG8xs9lp1buTVErSV0vakVaZcOfB3YWaU5eLSPdLez+PO9x9SfDzCICZ\nnQ9cBywCVgFfMbOMmWWALwOXA+cD1wfHAvx5cK5fAt4EPpFyvTtCpSR99a60rrZN6mmzM1P7QwwP\nZms+b03b2IpIz2jHZlBXAve5+9vu/hJwCLgo+Dnk7i+6+wngPuBKMzPgUuCB4P13AyNtqHfLVdqk\n55Lz5pB05/DhwSybr7mAzVdfQD44d2l/79LvocHZ3LFmCU9suJSNVyyasWlUpc/spNaQiKQr7eBx\no5k9bWZ3mdlwUJYHXg0dczgoiyt/FzDq7ifLynte1I5/Bpz7rhwP7i4kXsvx07GT3LxlL5u3H2T9\nyoW8vOlDfOHaC8hlM1MD7qVxla17CpG71d2xZslU4CmnnedE+kdDA+Zm9j3gPREvfRr4KvA5irnz\nPgd8AfjdRj6vhvqsA9YBzJs3L82PaomRpXl2vXKMe3b+aCpQOPDPLxyraxFgeYCA6psfxa361s5z\nIv2toeDh7h+o5Tgz+2vg28HTAjA39PI5QRkx5W8AQ2Y2K2h9hI8vr8+dwJ1QTMle42V0tB0Hjs4I\nFM24sGoD75W6oJTwT0RSm6prZme5+5Hg6VXAs8HjbcDfm9lfAmcDC4AfUOyRWWBm8ykGh+uA33Z3\nN7MdwNUUx0HWAg+lVe9Ok+Y4QqV9Pqp1QSkPlUh/S3Odx1+Y2RKKX5RfBn4fwN33m9n9wA+Bk8AN\n7j4BYGY3AtuBDHCXu+8PzvUp4D4zuw3YA3wtxXp3lGbui10rdUGJSDXaSbDDtTpnVV5dUCJ9TTsJ\n9ojSoPm9T76aegoSg2nbw4qIxGnHOg9JYOueAg/uLjQtcOSHcppqKyINU8ujw5QSIRZGx8jUkSnX\niJ+NFR7L0FRbEWmEgkcHaTRTbi6b4SMX5qe2bj09l8UMRo+PR06n1VRbEamXgkcHaSRT7oAV127s\nOHC0pkCgqbYi0giNeXSQeqfkZgaMSX/nHKX0IiIiaVHw6CADSTMdBiYmp3dvjY1P8Mn79zF/w8Os\n2PS4AomINJ26rdqkfIfAS86bw2QTZ+JG5bFSN5WINItaHm1QGhgvjI7hFG/wf7fzR6l9nvbaEJFm\nU/Bog3ZsIau9NkSkmRQ82qAdN3ItABSRZlLwaIN6buSD2QGGB7NTmzJ9scKmTOXj7loAKCLNpuDR\nAlv3FFix6fGp2U9JtpAtBYo/W/2rDM6ePr8haqfBXDbDR5fPm7b73+2rF2uwXESaSll1U9ZoVtyX\nN30o8hy5bIbbVy8GtFJcRJpHWXU7RCOD46VuqUpbxT6x4VIFCxFpOXVbpazewfFsxqbGKerZKlZE\nJE0KHimrd5bTabNnTbUo4s6hGVQi0i4KHimLGtSuxVtj4xXPoRlUItJOGvNIWan1cOu2/YyGAkI1\nZw/lpqUwOT2X5dTsQGx6dRGRVlLwaLKonFXf3nckUeDIZTNcct6caTOsRsfGyWUz3LFmiYKGiLSd\nuq2aKC5nVbXAUb4A8PbVi9lx4GjsDCsRkXZTy6OJ6p2We3x8EsemtSpu3rI38ljNsBKRTtBQy8PM\nrjGz/WY2aWbLyl67xcwOmdlBM1sZKl8VlB0ysw2h8vlm9mRQvsXMZgflpwTPDwWvn9tIndPUyI29\nvFWhGVYi0ska7bZ6FlgNfD9caGbnA9cBi4BVwFfMLGNmGeDLwOXA+cD1wbEAfw7c4e6/BLwJfCIo\n/wTwZlB+R3BcR2r0xh7eSVAzrESkkzUUPNz9OXeP6oS/ErjP3d9295eAQ8BFwc8hd3/R3U8A9wFX\nmpkBlwIPBO+/GxgJnevu4PEDwPuD4zvO+pULyWbqr5rB1K5/I0vz3L56sXJUiUhHSmvMIw/sDD0/\nHJQBvFpWfjHwLmDU3U9GHJ8vvcfdT5rZW8Hx/1L+oWa2DlgHMG/evKZcSGIRqcIGjJp2CXSK4yal\nADGyNK9gISIdqWrwMLPvAe+JeOnT7v5Q86tUP3e/E7gTiokRW/35m7cfZDwiSiTZXlYD4iLSDaoG\nD3f/QB3nLQBzQ8/PCcqIKX8DGDKzWUHrI3x86VyHzWwWcHpwfMdpxo1fA+Ii0g3SWuexDbgumCk1\nH1gA/AB4ClgQzKyaTXFQfZsX88LvAK4O3r8WeCh0rrXB46uBx71D88g3euPXgLiIdItGp+peZWaH\ngd8AHjaz7QDuvh+4H/gh8I/ADe4+EbQqbgS2A88B9wfHAnwK+CMzO0RxTONrQfnXgHcF5X8ETE3v\n7TSXnDen5mOHB7N8TJs2iUg/ZKAZAAAHo0lEQVSX0mZQDQinIjk1O8DY+GTN780P5Xhiw6Up1k5E\nJDltBpWy8t39kgQO0MC4iHQ35baqUyM7BIIGxkWkuyl41KmRloMGxkWk2yl41KmRloMGxkWk2yl4\n1Gn9yoXUk4hkeDCrwCEiXU8D5gmUb/SUdJ5aZsDYeMWiVOomItJKannUKGqjp0pWvPcMhnLZqefD\ng1m+cM0FanWISE9Qy6NGSWdXvfzGGHs3XpZijURE2kctjxpVa2mU0zoOEellCh41yiTcQkTrOESk\nlyl41GgiQRoXA63jEJGepuBRo3yNLQkDPrp8ngbGRaSnKXjUaP3KhWQHKndd5Ydy3LFmCbeNLG5R\nrURE2kOzrRI4GdN1NZTLamaViPQVBY8KSosCC6NjGJHbkwPw1th4K6slItJ2Ch4xylOuVxouPz20\nGFBEpB8oeISE049gUOsEq4SzeEVEup6CR6C8pZEkcdXocXVbiUh/0WyrQCObO2lBoIj0GwWPQL3p\nRLSxk4j0IwWPQD2th/xQThs7iUhfaih4mNk1ZrbfzCbNbFmo/FwzGzOzvcHP/wi9dqGZPWNmh8zs\nS2bF4WYzO8PMHjWz54Pfw0G5BccdMrOnzezXGqlznPUrF5LLZmo+3oAnNlyqwCEifanRlsezwGrg\n+xGvveDuS4KfPwiVfxX4PWBB8LMqKN8APObuC4DHgucAl4eOXRe8v+lGlua5ffXimhMgapxDRPpZ\nQ8HD3Z9z94O1Hm9mZwE/7+473d2BrwMjwctXAncHj+8uK/+6F+0EhoLzNN3I0jxfuPaCqmlINM4h\nIv0uzTGP+Wa2x8z+p5n9+6AsDxwOHXM4KAM4092PBI9/DJwZes+rMe9JxcnJ+Hm6w4NZjXOISN+r\nus7DzL4HvCfipU+7+0MxbzsCzHP3N8zsQmCrmdW8ebe7u5kl3SIcM1tHsWuLefPmJX07W/cUWP+N\nfRWXePzr+GTi84qI9JqqwcPdP5D0pO7+NvB28Hi3mb0A/DJQAM4JHXpOUAbwEzM7y92PBN1Srwfl\nBWBuzHvKP/dO4E6AZcuWJQ4+m7cfZLxCqwNgbHyCzdsPquUhIn0tlW4rM5tjZpng8S9SHOx+MeiW\n+qmZLQ9mWX0cKLVetgFrg8dry8o/Hsy6Wg68Fereaqpa13poi1kR6XeNTtW9yswOA78BPGxm24OX\n/gPwtJntBR4A/sDdjwWv/SHwN8Ah4AXgO0H5JuC3zOx54APBc4BHgBeD4/86eH8qap1BpZlWItLv\nGspt5e7fAr4VUf4g8GDMe3YBvxJR/gbw/ohyB25opJ61Wr9yIeu/sa9i15UBl5w3pxXVERHpWFph\nHjKyNM/may5gKJRiPVv2F3Lgwd0Ftu6JHHYREekLyqpbZmRpftpg+IpNj1MoG+PQoLmI9Du1PKqI\nGxzXoLmI9DMFjyriBsc1aC4i/UzBo4qohIlKTyIi/U5jHlWUxjVK29OePZRj/cqFGu8Qkb6m4FGD\n8kF0EZF+p24rERFJTMFDREQSU/AQEZHEFDxERCQxBQ8REUnMinkHe4+ZHQVeaXc96vBu4F/aXYkW\n0zX3B11zd/gFd6+a/bVng0e3MrNd7r6s3fVoJV1zf9A19xZ1W4mISGIKHiIikpiCR+e5s90VaANd\nc3/QNfcQjXmIiEhianmIiEhiCh4dwMw2m9kBM3vazL5lZkOh124xs0NmdtDMVrazns1kZteY2X4z\nmzSzZWWv9eQ1A5jZquC6DpnZhnbXJy1mdpeZvW5mz4bKzjCzR83s+eD3cDvr2ExmNtfMdpjZD4P/\nrv9rUN6z16zg0RkeBX7F3X8V+D/ALQBmdj5wHbAIWAV8xcwysWfpLs8Cq4Hvhwt7+ZqD6/gycDlw\nPnB9cL296G8p/vuFbQAec/cFwGPB815xEviku58PLAduCP5te/aaFTw6gLt/191PBk93AucEj68E\n7nP3t939JeAQcFE76ths7v6cux+MeKlnr5nidRxy9xfd/QRwH8Xr7Tnu/n3gWFnxlcDdweO7gZGW\nVipF7n7E3f938Pj/As8BeXr4mhU8Os/vAt8JHueBV0OvHQ7KelkvX3MvX1stznT3I8HjHwNntrMy\naTGzc4GlwJP08DVrM6gWMbPvAe+JeOnT7v5QcMynKTZ/72ll3dJSyzVLf3J3N7Oem+ppZj8HPAjc\n5O4/NbOp13rtmhU8WsTdP1DpdTP7z8B/BN7v78yfLgBzQ4edE5R1hWrXHKOrr7mKXr62WvzEzM5y\n9yNmdhbwersr1ExmlqUYOO5x928GxT17zeq26gBmtgr4b8CH3f146KVtwHVmdoqZzQcWAD9oRx1b\nqJev+SlggZnNN7PZFCcGbGtznVppG7A2eLwW6JnWpxWbGF8DnnP3vwy91LvXrEWC7Wdmh4BTgDeC\nop3u/gfBa5+mOA5ykmJT+DvRZ+kuZnYV8FfAHGAU2OvuK4PXevKaAczsg8AXgQxwl7t/vs1VSoWZ\n3Qu8j2JW2Z8AG4GtwP3APIoZr6919/JB9a5kZr8J/C/gGWAyKP5jiuMevXnNCh4iIpKUuq1ERCQx\nBQ8REUlMwUNERBJT8BARkcQUPEREJDEFDxERSUzBQ0REElPwEBGRxP4/Zi1LzMrRIJYAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def dataset(show=True):\n",
    "    X = np.arange(-25, 25, 0.1)\n",
    "    # Try changing y to a different function\n",
    "    y = X**3 + 20 + np.random.randn(500)*1000\n",
    "    if show:\n",
    "        plt.scatter(X, y)\n",
    "        plt.show()\n",
    "    return X, y\n",
    "\n",
    "X, y = dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKRpy65MNUlw"
   },
   "source": [
    "## Scaling Dataset\n",
    "\n",
    "The maximum value of y in the dataset goes upto 15000 and the minimum values is less than -15000. The range of y is very large which makes the convergence/loss reduction slower. So will we scale the data, scaling the data will help the model converge faster. If all the features and target are in same range, there will be symmetry in the curve of Loss vs weights/bias, which makes the convergence faster.\n",
    "\n",
    "We will do a very simple type of scaling, we will divide all the values of the data with the maximum values for X and y respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QzPpSAUaNTpi"
   },
   "outputs": [],
   "source": [
    "X, y = dataset()\n",
    "\n",
    "print(max(X), max(y), min(X), min(y))\n",
    "\n",
    "# TODO: divide array X and y by its max values\n",
    "X = #\n",
    "y = #\n",
    "\n",
    "print(max(X), max(y), min(X), min(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BqvlVbQDOoSd"
   },
   "source": [
    "This is not a great scaling method, but good to start. We will see many more scaling/normalizing methods later.\n",
    "\n",
    "**Try training the model with and without scaling and see the difference yourself.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jpzO5Eb207_i"
   },
   "source": [
    "## Linear Regression in TensorFlow\n",
    "\n",
    "If you are not sure about TF model definition check the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model)\n",
    "\n",
    "Refer the previous notebook, code is almost the same or you can even check the next notebook(solutions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioav6Ec6rpbJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X, y = dataset(show=False)\n",
    "X_scaled = # Scaled X value\n",
    "y_scaled = # Scaled y value\n",
    "\n",
    "model = # TF Dense Model\n",
    "\n",
    "# check how to define a optimizer here https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "optimizer = \n",
    "\n",
    "# train the model\n",
    "# compile the model\n",
    "\n",
    "tf_history = # fit the model\n",
    "\n",
    "\n",
    "plt.plot(tf_history.history['loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()\n",
    "\n",
    "mse = tf_history.history['loss'][-1]\n",
    "y_hat = model.predict(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.title('TensorFlow Model')\n",
    "plt.scatter(X_scaled, y_scaled, label='Data $(X, y)$')\n",
    "plt.plot(X_scaled, y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0)\n",
    "plt.xlabel('$X$', fontsize=20)\n",
    "plt.ylabel('$y$', fontsize=20)\n",
    "plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_yyuTj-RFin"
   },
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "So when the dataset is not linear, linear regression cannot learn the dataset and make good predictions. \n",
    "\n",
    "So we need a polynomial model which consideres the polynomial terms as well. So we need terms like $x^2$, $x^3$, ..., $x^n$ for the model to learn a polynomial of $n^{th}$ degree.\n",
    "\n",
    "$\\hat{y} = w_0 + w_1x + w_2x^2 + ... + w_nx^n$\n",
    "\n",
    "One down side of this model is that, We will have to decide the value of n. But this is better than a linear regression model. We can get an idea of the value of n by visualizing a dataset, but for multi variable dataset, we will have to try different values of n and check which is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x8u4rPzJTCJJ"
   },
   "source": [
    "## Polynomial Features\n",
    "\n",
    "you can calculate the polynomial features for each feature by programming it or you can try ```sklearn.preprocessing.PolynomialFeatures``` which allows us to make polynomial terms of our data.\n",
    "\n",
    "We will try degree 2, 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oZbYgRFcT9q6"
   },
   "outputs": [],
   "source": [
    "X, y = dataset(show=False)\n",
    "X_scaled = X/max(X)\n",
    "y_scaled = y/max(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDHF3FLBRBTK"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2) \n",
    "                                        \n",
    "X_2 = poly.fit_transform(X_scaled.reshape(-1,1))\n",
    "print(X_2.shape)\n",
    "print(X_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tp0jMLxPTxs2"
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    "# get the 3rd degree terms\n",
    "\n",
    "poly = # poly features\n",
    "                                        \n",
    "X_3 = # 3rd degree terms\n",
    "print(X_3.shape)\n",
    "print(X_3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0V5pFUWT4kg"
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    "# get the 4th degree terms\n",
    "\n",
    "poly = # poly features\n",
    "                                        \n",
    "X_4 = # 4th degree terms\n",
    "print(X_4.shape)\n",
    "print(X_4[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7rzpVB3VLN7"
   },
   "source": [
    "The PolynomialFeatures returns $[1, x, x^2, x^3,...]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mcdv2RDhUF7C"
   },
   "source": [
    "# Task - 2\n",
    "\n",
    "- Train a model with polynomial terms in the dataset.\n",
    "- Visualize the prediction of the model\n",
    "\n",
    "\n",
    "The code remains the same except, the no of input features will be 3, 4, 5 respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WCVRwEynVxYL"
   },
   "source": [
    "## Tensorflow Model with 2nd Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJXuJi6UUBZO"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "model = # TF Dense Model\n",
    "\n",
    "# check how to define a optimizer here https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "optimizer = \n",
    "\n",
    "# train the model\n",
    "# compile the model\n",
    "\n",
    "tf_history = # fit the model\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(tf_history.history['loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()\n",
    "\n",
    "mse = tf_history.history['loss'][-1]\n",
    "y_hat = model.predict(X_2)\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.title('TensorFlow Model')\n",
    "plt.scatter(X_2[:, 1], y_scaled, label='Data $(X, y)$')\n",
    "plt.plot(X_2[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0)\n",
    "plt.xlabel('$X$', fontsize=20)\n",
    "plt.ylabel('$y$', fontsize=20)\n",
    "plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovvrgy0_fcRM"
   },
   "source": [
    "Why is the polynomial regression with 2-d features look like a straight line?\n",
    "\n",
    "Well because the model thinks that a straight line(look like, we can't be sure if its a straight like, it can a parabola as well) better fits the dataset than a parabola. If you train the model for less epochs you can notice the model tries to fit the data with a parabola(2-d) but it improves as it moves to a line.\n",
    "\n",
    "Train the same model for may be 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wlJYRqnMVjCu"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "model = # TF Dense Model\n",
    "\n",
    "# check how to define a optimizer here https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "optimizer = \n",
    "\n",
    "# train the model for 50 epochs\n",
    "# compile the model\n",
    "\n",
    "tf_history = # fit the model\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(tf_history.history['loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()\n",
    "\n",
    "mse = tf_history.history['loss'][-1]\n",
    "y_hat = model.predict(X_2)\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.title('TensorFlow Model')\n",
    "plt.scatter(X_2[:, 1], y_scaled, label='Data $(X, y)$')\n",
    "plt.plot(X_2[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0)\n",
    "plt.xlabel('$X$', fontsize=20)\n",
    "plt.ylabel('$y$', fontsize=20)\n",
    "plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PN99ty_GER0Z"
   },
   "source": [
    "You can clearly see that the model tries to fit the data with a parabola which doen't seem to fit well, so it changes the parameters to fit a line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gV2fxw9rV3aA"
   },
   "source": [
    "## Tensorflow Model with 3rd Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVY5HcC7V3aO"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model = # TF Dense Model\n",
    "\n",
    "# check how to define a optimizer here https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "optimizer = \n",
    "\n",
    "# train the model \n",
    "# compile the model\n",
    "\n",
    "tf_history = # fit the model\n",
    "\n",
    "plt.plot(tf_history.history['loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()\n",
    "\n",
    "mse = tf_history.history['loss'][-1]\n",
    "y_hat = model.predict(X_3)\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.title('TensorFlow Model')\n",
    "plt.scatter(X_3[:, 1], y_scaled, label='Data $(X, y)$')\n",
    "plt.plot(X_3[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0)\n",
    "plt.xlabel('$X$', fontsize=20)\n",
    "plt.ylabel('$y$', fontsize=20)\n",
    "plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djQgZARjgYfI"
   },
   "source": [
    "3-D features perfectly fit the data with a 3rd degree polynomial as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zppsoHcaV4qK"
   },
   "source": [
    "# Tensorflow Model with 4th Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oYMBxfHnV4qO"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model = # TF Dense Model\n",
    "\n",
    "# check how to define a optimizer here https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "optimizer = \n",
    "\n",
    "# train the model\n",
    "# compile the model\n",
    "\n",
    "tf_history = # fit the model\n",
    "\n",
    "plt.plot(tf_history.history['loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()\n",
    "\n",
    "mse = tf_history.history['loss'][-1]\n",
    "y_hat = model.predict(X_4)\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.title('TensorFlow Model')\n",
    "plt.scatter(X_4[:, 1], y_scaled, label='Data $(X, y)$')\n",
    "plt.plot(X_4[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0)\n",
    "plt.xlabel('$X$', fontsize=20)\n",
    "plt.ylabel('$y$', fontsize=20)\n",
    "plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bHIymsWZ_kxw"
   },
   "source": [
    "4th degree poly-regression also did a good job in fitting the data as it also have the 3rd degree terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BbS6DGuU_mpi"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model = # TF Dense Model\n",
    "\n",
    "# check how to define a optimizer here https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "optimizer = \n",
    "\n",
    "# train the model for 50 epochs\n",
    "# compile the model\n",
    "\n",
    "tf_history = # fit the model\n",
    "\n",
    "plt.plot(tf_history.history['loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()\n",
    "\n",
    "mse = tf_history.history['loss'][-1]\n",
    "y_hat = model.predict(X_4)\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.title('TensorFlow Model')\n",
    "plt.scatter(X_4[:, 1], y_scaled, label='Data $(X, y)$')\n",
    "plt.plot(X_4[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0)\n",
    "plt.xlabel('$X$', fontsize=20)\n",
    "plt.ylabel('$y$', fontsize=20)\n",
    "plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5aMu5aQ_8y_"
   },
   "source": [
    "If you run the 4th degree poly-regression for fewer epochs, you can notice, the model tries to fit a 4th(or higher than 3rd) degree polynomial but as the loss is high, the model changes it parameters to set the 4th degree terms to almost 0 and thus giving a 3rd degree polynomial as you train for more epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VC4GQ-bQXD9v"
   },
   "source": [
    "\n",
    "\n",
    "This is polynomial regression. Yes, its easy. But one issue, as this was a toy dataset we know its a 3rd degree data, so we tried 2,3,4. But when the data is multi dimensional we cannot visualize the dataset, so its difficult to decide the degree. This is why you will see Neural Networks are awesome. They are End-End, they do not need several feature extraction from our side, they can extract necessary features of their own.\n",
    "\n",
    "\n",
    "**Make a Higher degree (4th/5th degree) data and try polynomial regression on it. Also try different functions like exponents, trignometric..etc.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "1_2_1_Assignment_Polynomial_Regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "ipub": {
   "titlepage": {
    "author": "Shangeth Rajaa",
    "email": "shangethrajaa@gmail.com",
    "website": "shangeth.com"
   }
  },
  "kernelspec": {
   "display_name": "Python3 (dlenv)",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
