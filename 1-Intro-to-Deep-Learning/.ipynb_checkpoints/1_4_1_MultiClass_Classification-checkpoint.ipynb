{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_4_MultiClass_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0j5dFed2nzQ"
   },
   "source": [
    "<h1><a href='https://shangeth.com/google-ml-academy/'>Google ML Academy 2019</a></h1>\n",
    "<h3>Instructor: <a href='https://shangeth.com/'>Shangeth Rajaa</a></h3>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJhS9-RC2trS"
   },
   "source": [
    "# Multi Class Classification\n",
    "\n",
    "In the previous notebeook we used logistic regression for Binary Classification, now we will see how to train a classifier model for Multi-Class Classification.\n",
    "\n",
    "**What is Multi-Class Classification?** \n",
    "If the target values have n discrete classification classes ie: y can take discrete value from 0 to n-1. If $y \\in \\{0, 1, 2, 3, ..., n-1\\}$, then the classification task is n-Multi-Class.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/972/1*SwXHlCzh-d9UqHOglp3vcA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f1PH1I3T7R8Y"
   },
   "source": [
    "# Task - 1 \n",
    "\n",
    "Create a 3-Multi-Class dataset with sklearn.datasets and visualize it.\n",
    "\n",
    "It's very easy, use the same code form previous notebook and make changes for 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xVZ9va7Y5OZs",
    "outputId": "d8eb882f-56ef-4190-ead0-36aa9b08160c"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# make dataset with 2 input features and 3 output classes\n",
    "X, y = \n",
    "\n",
    "X.shape, y.shape, set(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HV8IowwM7wOH"
   },
   "source": [
    "If you made 3 centers, you can see ```set(y)``` will return ```{0, 1, 2}```. where 0 represent the first class, 1 represent second and 2 represents the third class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ksMH7t0S7ndy",
    "outputId": "76f0c9d8-d44c-482d-8b43-f7e00b76ce0b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# getting the index of each class\n",
    "class_0 = \n",
    "class_1 = \n",
    "class_2 =\n",
    "\n",
    "# getting data from index\n",
    "X_0 = \n",
    "X_1 = \n",
    "X_2 = \n",
    "\n",
    "X_0.shape, X_1.shape, X_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "colab_type": "code",
    "id": "C9Jyj7zm8NcZ",
    "outputId": "9c43e33c-1507-4143-fc53-5d515fd7d9f2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make scatter plot of each class data with different colour and marker\n",
    "plt.figure(figsize=(12,9))\n",
    "\n",
    "plt.xlabel('$x1$', fontsize=20)\n",
    "plt.ylabel('$x2$', fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BFJ9O1gLBy7y"
   },
   "source": [
    "# How to get the probabilities? : SoftMax\n",
    "\n",
    "Now the target is going to be $y \\in \\{0, 1, 2\\}$, so sigmoid cannot be used here, as sigmoid will convert any number to range 0 to 1 , so it can only be used for binary classification.\n",
    "\n",
    "We need a function which converts the scores/logits of linear mapping into probabilities for all n classes.\n",
    "\n",
    "That function should have some properties:\n",
    "\n",
    "- all probabilities should be >0\n",
    "- probabilities should be in range $[0,1]$\n",
    "- some of all class probabilities = 1\n",
    "\n",
    "<br>\n",
    "One possible function can be class_logit/sum of all class logits. Lets try it\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "$logits = [-100, 40, -10]$, right now don't bother how do we get 3 logits, we will discuss it below.\n",
    "\n",
    "$probabilities = [-100/(-100+40-10), 40/(-100+40-10), -10/(-100+40-10)]$\n",
    "\n",
    "$probabilities = [100/70, -40/70, 10/70]$ \n",
    "\n",
    "you can see this example satisfies only the third property(sum=1). So we need a function which gives positive numbers. Exponential function can help us.\n",
    "\n",
    "$Logits = [l_0, l_1, l_2, ...., l_{n-1}]$\n",
    "\n",
    "$Probabilities = [\\dfrac{e^{l_0}}{e^{l_1} + e^{l_2}+ ... + e^{l_{n-1}}}, \\dfrac{e^{l_1}}{e^{l_1} + e^{l_2}+ ... + e^{l_{n-1}}}, ..., \\dfrac{e^{l_{n-1}}}{e^{l_1} + e^{l_2}+ ... + e^{l_{n-1}}}]$\n",
    "\n",
    "This function is called Softmax, and this gives the probability that a data belongs to class j, given the logits.\n",
    "\n",
    "$P(y=j|z) = \\dfrac{e^{z_j}}{\\sum_{i=0}^{n-1}e^{z_i}}$\n",
    "\n",
    "Let's code softmax function in Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqhcZG0C8f5F"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "  exp = np.exp(x)\n",
    "  exp_sum = exp.sum(axis=1).reshape(-1,1)\n",
    "  return exp/exp_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FvOLbFM_SdtE",
    "outputId": "35047b18-f342-4555-c965-373329f7e6fa"
   },
   "outputs": [],
   "source": [
    "x = np.array([[22, 40, 10]])\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fT_IVhgS25P"
   },
   "source": [
    "Now we know, replacing sigmoid with with softmax will help in the case of multi class classification. This softmax model is also called **Softmax Regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvUrCWTgUgei"
   },
   "source": [
    "# Loss Function\n",
    "\n",
    "As we have already seen, for classification task we will use Cross Entropy loss.\n",
    "The prediction of softmax regression $\\hat{y} = [0.129, 0.8, 0.071]$, whereas the true label will be one of $y \\in \\{0, 1, 2\\}$. We cannot directly use Cross Entropy loss with $\\hat{y}$ and $y$. \n",
    "\n",
    "So we convert the true label into One-Hot Encoding form. One hot encoding is a vector representation of the label which has '1' at the index corresponding to the label and '0' elsewhere.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let $y \\in \\{0, 1, 2, 3, 4\\}$, then \n",
    "- '4' is represented as $[0, 0, 0, 0, 1]$\n",
    "- '3' is represented as $[0, 0, 0, 1, 0]$\n",
    "- '2' is represented as $[0, 0, 1, 0, 0]$\n",
    "- '1' is represented as $[0, 1, 0, 0, 0]$\n",
    "- '0' is represented as $[1, 0, 0, 0, 0]$\n",
    "\n",
    "Let's code the label to one hot conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "JE0jCm1RSiZN",
    "outputId": "2715c8a9-f363-4f9e-ac24-ee3399588534"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_one_hot(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "num_classes = 5\n",
    "labels = np.array([0, 1, 2, 3, 4])\n",
    "\n",
    "to_one_hot(labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVPirthCcSbF"
   },
   "source": [
    "You can also use sklearn.preprocessing.OneHotEncoder to convert labels to one hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "1QDMIWlMZY7u",
    "outputId": "95d239d4-a793-459d-8e20-f47d27277ec1"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "labels = np.array([[0], \n",
    "                   [1], \n",
    "                   [2], \n",
    "                   [3], \n",
    "                   [4]])\n",
    "\n",
    "OneHotEncoder(categories='auto').fit_transform(labels).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VeSyPE14fqfS"
   },
   "source": [
    "Keras also have some utils functions which can help in one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "0q4RawnBfwwI",
    "outputId": "d2527ed2-aee1-40a2-ae4e-29f9aff23e8a"
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical   \n",
    "\n",
    "labels = np.array([0, 1, 2, 3, 4])\n",
    "to_categorical(labels, num_classes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nGRdPE19m18x"
   },
   "source": [
    "# Softmax Regression Model\n",
    "\n",
    "From what we discussed so far, if the number of classes = 3, then we expect model to give a prediction $\\hat{y} = Softmax(z)$ and $z$ will be like $z = [-10, 20, 5]$(Example).\n",
    "\n",
    "$z = X.W + b$ will only give one number like $z=[4]$ in logistic regression. But now we are using softmax regression which expect a model which gives 3 output for a 3 class classifier. \n",
    "\n",
    "If the no of input features = 2 and no of out[ut classes = 3\n",
    "So we will use 3 linear classifier. \n",
    "\n",
    "$z_1 = X.W_1 + b_1$, $z_2 = X.W_2 + b_2$, $z_3 = X.W_3 + b_3$ which can be combined together with\n",
    "\n",
    "z = $\\begin{bmatrix}z_1&z_2&z_3\\\\ \\end{bmatrix}$ \n",
    "\n",
    "$ z = X . W + b $\n",
    "\n",
    "$W = \\begin{bmatrix}W_1&W_2&W_3\\\\ \\end{bmatrix} $\n",
    "\n",
    "$b = \\begin{bmatrix}b_1&b_2&b_3\\\\ \\end{bmatrix}$\n",
    "\n",
    "each $W_i = \\begin{bmatrix}W_{i1}\\\\W_{i2}\\\\W_{}i3\\\\ \\end{bmatrix} $\n",
    "\n",
    "so the final $W = \\begin{bmatrix}W_{11}&W_{12}&W_{13}\\\\ W_{21}&W_{22}&W_{23}\\\\ \\end{bmatrix}$\n",
    "\n",
    "<br><hr><br>\n",
    "Let $X = \\begin{bmatrix}x_1&x_2\\\\ \\end{bmatrix}$\n",
    "\n",
    "$z = \\begin{bmatrix}x_1&x_2\\\\ \\end{bmatrix} . \\begin{bmatrix}W_{11}&W_{12}&W_{13}\\\\ W_{21}&W_{22}&W_{23}\\\\ \\end{bmatrix} + \\begin{bmatrix}b_1&b_2&b_3\\\\ \\end{bmatrix}$\n",
    "\n",
    "<br>\n",
    "\n",
    "- For 1 data, $(1,2).(2,3) + (1,3) = (1,3)$\n",
    "- For n data, $(n,2).(2,3) + (1,3) = (n,3)$, b will be added to all n data, this is called broadcasting.\n",
    "\n",
    "<br>\n",
    "Frameworks like Tensorflow, PyTorch will take care of this matrix form of $W$ and $b$ for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ex5CKUYSeat7"
   },
   "source": [
    "# Task-2\n",
    "\n",
    "Train a Softmax Regression with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l-ZMyD0RcjXs",
    "outputId": "879b302c-5a8d-4e4c-a736-d850bd041c16"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=1000, n_features=2, centers=3, random_state=42)\n",
    "X.shape, y.shape, set(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HtHahESle2sD"
   },
   "source": [
    "We need to convert the labels into one hot vectors to train the model. Let's use keras to_categorical function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "pnUUTh72exBc",
    "outputId": "153a3c24-4041-4ee2-eeb2-8d7cd3d67319"
   },
   "outputs": [],
   "source": [
    "print(y.shape)\n",
    "\n",
    "y = to_categorical(y, num_classes=3)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iPcOfy1ThSxJ"
   },
   "source": [
    "## Train-Validation-Test Split\n",
    "\n",
    "So far we had a dataset and we used it for training and checked the accuracy/loss to see the performance. But its not the right way to check the preformance of a model. Usually any dataset is split into 3 parts namely Train-Validation-Test.\n",
    "\n",
    "### train set\n",
    "This dataset is used to train the model. If the training is good, metrics of this dataset will be always good. Almost 60-70% of dataset is given to training set.\n",
    "\n",
    "### validation set\n",
    "After every epoch(generally) of training, metrics of this dataset is checked to ensure, the model is also performing good on unseen data as much as it performs on the training dataset. If the model performs well on training dataset but not good on validation set, it means the model has a problem called 'Overfitting' whcih we will look in more detail later. Some hyper parameters are adjusted to make the model perform well in validation set as well during training. 10-20% of data is given to validation set.\n",
    "\n",
    "### testing set\n",
    "After the training is over for n epochs, when the model performs good in both training and validation sets, a final check is done to see the performance of the model on new unseen dataset. 20-30% of data is given to Test set.\n",
    "\n",
    "\n",
    "The percentage numbers depends on the total number of data we have access to, which you will understand as you work on more projects.\n",
    "\n",
    "We can split the dataset into train-test using ```sklearn.model_selection.train_test_split```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HKvcrb_ifal6",
    "outputId": "8b537fad-7182-4aa6-b865-819a47da1715"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "# test_size is the percent of split 0.2 means 20% of data is for testset.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nO3hmN_GlFXw"
   },
   "source": [
    "## Tensorflow Model\n",
    "\n",
    "- Make a Dataset with 2 input features, 3 output classes\n",
    "- one hot encode y\n",
    "- Split Dataset into Train-Validation-Test\n",
    "- Train Model with Validation Dataset, check the [docs](https://keras.io/models/model/) on how to use validation data.\n",
    "- Do not forget to change the input shape and units in Dense layer and use \"softmax\" activation ```keras.layers.Activation('softmax')```\n",
    "- Use \"categorical_crossentropy\" loss\n",
    "- predict X_test with the trained model, refer the docs(model.predict function)\n",
    "- convert the prediction of X_test and y_test from one-hot to labels using np.argmax(pred, axis=1)\n",
    "- use sklearn.metrics.accuracy_score on prediction of X_test and y_test to find the accuracy on Test set.\n",
    "\n",
    "\n",
    "If you are stuck somewhere before checking the solution\n",
    "- check the keras docs\n",
    "- google search will help and you will learn many possible solutions in stackoverflow\n",
    "- then go to the solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "J0wAE4Kdj6f_",
    "outputId": "48a07218-afc0-450c-b823-31ccfb0ebd16"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Make the Dataset\n",
    "num_classes = 3\n",
    "num_input_features = 2\n",
    "X, y = \n",
    "\n",
    "# to categorical\n",
    "y = \n",
    "\n",
    "# train-test split\n",
    "# 20% of dataset to testset\n",
    "X_train, X_test, y_train, y_test = \n",
    "\n",
    "# train-validation split\n",
    "# 20% of trainset to valset\n",
    "X_train, X_val, y_train, y_val = \n",
    "\n",
    "# Model\n",
    "model = \n",
    "model.compile()\n",
    "tf_history = model.fit()\n",
    "\n",
    "# Prediction for Test set with trained Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_test_pred = \n",
    "test_accuracy = \n",
    "\n",
    "print('\\nTest Accuracy = ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DM7EuoNGx6SG"
   },
   "source": [
    "Train the model with different\n",
    "- no of input features\n",
    "- no of output classes\n",
    "- no of data\n",
    "- split of train-validation-test\n",
    "- epochs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "1_4_MultiClass_Classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
