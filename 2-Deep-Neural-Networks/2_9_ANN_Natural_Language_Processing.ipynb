{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_9_ANN_Natural_Language_Processing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shangeth/Google-ML-Academy/blob/master/2-Deep-Neural-Networks/2_9_ANN_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UouPVtDQhkcJ",
        "colab_type": "text"
      },
      "source": [
        "<h1><a href='https://shangeth.com/courses/'>Google Explore ML Academy</a></h1>\n",
        "<h3>Instructor: <a href='https://shangeth.com/'>Shangeth Rajaa</a></h3>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQQ-UCMDh1cC",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "We are going to classify a movie review as Positive or Negative review given a text review.\n",
        "\n",
        "![](https://cfml.se/img/blog/sentiment_classification/top_img.png)\n",
        "\n",
        "\n",
        "We'll use the IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZxY4GN_isbf",
        "colab_type": "text"
      },
      "source": [
        "# IMDB Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpvoaYhulUUg",
        "colab_type": "text"
      },
      "source": [
        "## Download the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTXDK65Ohkz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "imdb = tf.keras.datasets.imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxk5GQj6lfEs",
        "colab_type": "text"
      },
      "source": [
        "## Explore the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzObY37tlWlB",
        "colab_type": "code",
        "outputId": "769421a6-443c-409f-a564-080e485fb777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data.shape, train_labels.shape, test_data.shape, test_labels.shape"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000,), (25000,), (25000,), (25000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD18fhfzlboD",
        "colab_type": "code",
        "outputId": "527fdce9-9804-473c-eec4-e2df7e16ebcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_data[0])"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAeZVFbulwdz",
        "colab_type": "text"
      },
      "source": [
        "Each of these numbers correspond to a word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR8dQ0Hblhmk",
        "colab_type": "code",
        "outputId": "406c6f1e-1164-43d0-9c3a-adea621d5842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_2_int = tf.keras.datasets.imdb.get_word_index(path='imdb_word_index.json')\n",
        "\n",
        "word_2_int['hello'], word_2_int['world']"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4822, 179)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPkLjOd8xeP7",
        "colab_type": "code",
        "outputId": "02340fb9-0b69-4fe5-bc9e-b0960a8f9c4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(word_2_int)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88584"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRH-CO2YwRaI",
        "colab_type": "code",
        "outputId": "571d6ea0-93c2-4400-cf4d-dd2c42ea9fd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def sentence_2_int(sentence):\n",
        "    sentence_2_int_list = []\n",
        "    for i in sentence.lower().split(' '):\n",
        "        sentence_2_int_list.append(word_2_int[i])\n",
        "    return sentence_2_int_list\n",
        "\n",
        "sentence = \"Worst movie i've ever seen\"\n",
        "sentence_2_int(sentence)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[246, 17, 204, 123, 107]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYKzHMr6x7LT",
        "colab_type": "text"
      },
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ7BIjAWx-b_",
        "colab_type": "text"
      },
      "source": [
        "### Padding\n",
        "\n",
        "Every sentence will be off different length, to pass the sentences through the ANN model, we need to have a fixed length data.\n",
        "So we\n",
        "- pad small sentence\n",
        "- cut very long sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWpZ5HMBxcTT",
        "colab_type": "code",
        "outputId": "fd8823fd-1d1b-4835-f2bf-26aabdcfec08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pad_value = 0\n",
        "sentence_len = 100\n",
        "\n",
        "train_data = tf.keras.preprocessing.sequence.pad_sequences(train_data,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=sentence_len)\n",
        "\n",
        "test_data = tf.keras.preprocessing.sequence.pad_sequences(test_data,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=sentence_len)\n",
        "\n",
        "train_data.shape, train_labels.shape, test_data.shape, test_labels.shape"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000, 100), (25000,), (25000, 100), (25000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU-SAIKO6xUs",
        "colab_type": "text"
      },
      "source": [
        "# ANN Model\n",
        "\n",
        "We will use a new layer into our model called the Embedding layer. \n",
        "\n",
        "The words of each sentence are now represented with numbers, we cannot directly feed those number into the model, we need to convert the numbers into vectors for the model to understand what word it is?\n",
        "\n",
        "but how do we decide the vectors? we leave that to the model. \n",
        "\n",
        "So embedding layer, takes in an integer and converts it into a vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duIjjy256SQ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "ba43492f-c4b3-4a7f-ac70-c0f65d7b7156"
      },
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=sentence_len),\n",
        "                             # the model will take as input an integer matrix of size (batch, input_length).\n",
        "                             # now model.output_shape == (None, 100, 16), where None is the batch dimension.\n",
        "                             tf.keras.layers.Dropout(0.4),\n",
        "                             tf.keras.layers.Flatten(),\n",
        "                             tf.keras.layers.Dense(units=sentence_len*embedding_dim), \n",
        "                             tf.keras.layers.Activation('relu'),\n",
        "                             tf.keras.layers.Dropout(0.4),\n",
        "                             tf.keras.layers.Dense(units=500),\n",
        "                             tf.keras.layers.Activation('relu'),\n",
        "                             tf.keras.layers.Dropout(0.4),\n",
        "                             tf.keras.layers.Dense(units=1), \n",
        "                             tf.keras.layers.Activation('sigmoid')\n",
        "                             ])\n",
        "model.summary()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 16)           160000    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100, 16)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1600)              2561600   \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               800500    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 501       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 3,522,601\n",
            "Trainable params: 3,522,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8o8WlKj8WqM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "58fe2b85-d308-46c8-d8c4-b5a1dd9f5e64"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "tf_history = model.fit(train_data, train_labels, batch_size=2000, epochs=10, verbose=True, validation_data=(test_data, test_labels))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 12s 489us/sample - loss: 0.6917 - acc: 0.5162 - val_loss: 0.6860 - val_acc: 0.5558\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 12s 485us/sample - loss: 0.6434 - acc: 0.6450 - val_loss: 0.5741 - val_acc: 0.7036\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 12s 482us/sample - loss: 0.4729 - acc: 0.7753 - val_loss: 0.4278 - val_acc: 0.8019\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 12s 487us/sample - loss: 0.3406 - acc: 0.8524 - val_loss: 0.3833 - val_acc: 0.8292\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 12s 483us/sample - loss: 0.2646 - acc: 0.8873 - val_loss: 0.3691 - val_acc: 0.8388\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 12s 482us/sample - loss: 0.2038 - acc: 0.9180 - val_loss: 0.3913 - val_acc: 0.8380\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 12s 484us/sample - loss: 0.1641 - acc: 0.9350 - val_loss: 0.4083 - val_acc: 0.8380\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 12s 483us/sample - loss: 0.1318 - acc: 0.9490 - val_loss: 0.4307 - val_acc: 0.8374\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 12s 484us/sample - loss: 0.1029 - acc: 0.9595 - val_loss: 0.4761 - val_acc: 0.8348\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 12s 485us/sample - loss: 0.0871 - acc: 0.9660 - val_loss: 0.4966 - val_acc: 0.8368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94ejGEOy8pS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('trained_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X7FNS92T3in",
        "colab_type": "text"
      },
      "source": [
        "# Model Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls_4q1ZlT7Dg",
        "colab_type": "text"
      },
      "source": [
        "## Load trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTc74n7ST1PV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "52ba8573-7815-44fb-f9dd-27d663b13cba"
      },
      "source": [
        "trained_model = tf.keras.models.load_model('trained_model.h5')\n",
        "\n",
        "trained_model.summary()"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 16)           160000    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100, 16)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1600)              2561600   \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               800500    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 501       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 3,522,601\n",
            "Trainable params: 3,522,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO3a7bWAUWpT",
        "colab_type": "text"
      },
      "source": [
        "## Sentence to Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlgHP_aqUFbh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "4fbcb613-b20f-4623-a8c4-34d14dc164fc"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sentence_2_int(sentence):\n",
        "    sentence_2_int_list = []\n",
        "    for i in sentence.lower().split(' '):\n",
        "        sentence_2_int_list.append(word_2_int[i])\n",
        "    arr = np.array(sentence_2_int_list).reshape(1,-1)\n",
        "    arr = tf.keras.preprocessing.sequence.pad_sequences(arr, value=0, padding='post', maxlen=100)\n",
        "    return arr\n",
        "\n",
        "sentence = \"Worst movie i've ever seen\"\n",
        "sentence_2_int(sentence)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[246,  17, 204, 123, 107,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtIic5ygUZ-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b725186-43d6-4316-cd35-7342b81c34e4"
      },
      "source": [
        "model.predict(sentence_2_int(sentence))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.22403198]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5q6OdbLVDTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_2_prediction(sentence):\n",
        "    vector = sentence_2_int(sentence)\n",
        "    prob = model.predict(vector)\n",
        "    prediction = prob > 0.5\n",
        "    if prediction == 1:\n",
        "        print('Positive Review :D')\n",
        "    else:\n",
        "        print('Negative Review :(')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHZWcw7oVfeC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8507a295-dca4-4cd1-e983-fbd1c3157b88"
      },
      "source": [
        "sentence = 'Good Movie i really enjoyed it'\n",
        "\n",
        "sentence_2_prediction(sentence)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Review :D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhPC1NaeVklf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae7a9113-2ddc-458c-85c0-3181faa8fa57"
      },
      "source": [
        "sentence = 'worst movie'\n",
        "\n",
        "sentence_2_prediction(sentence)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Negative Review :(\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW__q9HwWAuM",
        "colab_type": "text"
      },
      "source": [
        "If you try many different sentences, you may notice the model actually doen't perform well. There may be many reasons for it.\n",
        "\n",
        "- vocabulary size\n",
        "- sequence length(no of words in a sentence for padding)\n",
        "- model architecture\n",
        "- embedding dim\n",
        "\n",
        "Train the model by changing all the above to improve its performance.\n"
      ]
    }
  ]
}